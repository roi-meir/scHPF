#! /usr/bin/env python

import os
import sys
import argparse
import json
import time
from functools import partial

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")

import numpy as np
import pandas as pd
from scipy.io import mmread, mmwrite
from scipy.sparse import coo_matrix
import joblib

from schpf import scHPF, run_trials, run_trials_pool
from schpf.util import max_pairwise_table, mean_cellscore_fraction_list
from schpf.preprocessing import load_coo, load_and_filter, load_like
from schpf.preprocessing import split_validation_cells

def _parser():
    # usage = """scHPF <command> [<args>]

# The most commonly used scHPF commands are:
    # prep    Prepare data
    # train   Train a model from data
    # score   Get cell-scores, gene-scores, and other data

# Some advanced scHPF commands are:
    # prep-like    Prepare data with the same genes & order as other data
    # project      Project data onto a pre-trained model
    # train-pool   Train a model, parallelized at the level of trials
    # """
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest='cmd')

    ### Preprocess command
    prep = subparsers.add_parser('prep',
            help='Prepare data for training')
    # data
    prep.add_argument('-i', '--input', required=True,
            help='Input data. Currently accepts either: (1) a whitespace-'
            'delimited gene by cell UMI count matrix with 2 leading columns '
            'of gene attributes (ENSEMBL_ID and GENE_NAME respectively), or '
            '(2) a loom file with at least one of the row attributes '
            '`Accession` or `Gene`, where `Accession` is an ENSEMBL id and '
            '`Gene` is the name.'
            )
    prep.add_argument('-o', '--outdir', 
            help='Output directory. Does not need to exist.')
    prep.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # gene filtering criteria
    prep.add_argument('-m', '--min-cells', type=float, default=0.01, 
            help='Minimum number of cells in which we must observe at '
            'least one transcript of a gene for the gene to pass '
            'filtering. If 0 <`min_cells`< 1, sets threshold to be '
            '`min_cells` * ncells, rounded to the nearest integer.'
            ' [Default 0.01]')
    prep.add_argument('-w', '--whitelist', default='',
            help='Tab-delimited file where first column contains ENSEMBL gene '
            'ids to accept, and second column contains corresponding gene '
            'names. If given, genes not on the whitelist are filtered from '
            'the input matrix. Superseded by blacklist. Optional.')
    prep.add_argument('-b', '--blacklist', default='',
            help='Tab-delimited file where first column contains ENSEMBL gene '
            'ids to exclude, and second column is the corresponding gene name. '
            'Only performed if file given. Genes on the blacklist are '
            'excluded even if they are also on the whitelist. Optional.')

    # optional selection of cells for validation set
    prep.add_argument('-nvc', '--n-validation-cells', type=int, default=0,
            help='Number of cells to randomly select for validation.')
    prep.add_argument('-vgid', '--validation-group-ids', default=None,
            help= 'Single column file of cell group ids readable with '
            ' np.readtxt. If `--n-validation-cells` is > 0, cells will be '
            ' randomly selected approximately evenly across the groups in '
            ' this file, under the constraint that at most'
            ' `--validation-min-group-frac` * (ncells in group) are selected'
            ' from every group.')
    prep.add_argument('--validation-max-group-frac', type=float, default=0.5,
            help='If `-nvc`>0 and `validation-group-ids` is a valid file, at'
            ' most `validation-min-group-frac`*(ncells in group) cells are'
            ' selected from each group.')

    # other options
    prep.add_argument('--filter-by-gene-name', default=False, 
            action='store_true', help='Use gene name rather than ENSEMBL'
            ' id to filter (with whitelist or blacklist).  Useful for'
            ' datasets where only gene symbols are given. Applies to both'
            ' whitelist and blacklist. Used by default when input is a loom'
            ' file (unless there is an Accession attribute in the loom).')
    prep.add_argument('--no-split-on-dot', default=False, action='store_true',
            help='Don\'t split gene symbol or name on period before '
            'filtering whitelist and blacklist. We do this by default for '
            'ENSEMBL ids.')


    #### Prepare like
    prep_like = subparsers.add_parser('prep-like', 
            help='Prepare a data set like another (ie with the same genes in'
            ' the same order)')
    # data
    prep_like.add_argument('-i', '--input', required=True,
            help='Input data to format. Currently accepts either: (1) a'
            ' whitespace-delimited gene by cell UMI count matrix with 2'
            ' leading columns of gene attributes (ENSEMBL_ID and GENE_NAME'
            ' respectively), or (2) a loom file with at least one of the row'
            ' attributes `Accession` or `Gene`, where `Accession` is an'
            ' ENSEMBL id and `Gene` is the name.')
    prep_like.add_argument('-r', '--reference', required=True,
            help='Two-column tab-delimited file of ENSEMBL ids and gene names'
            ' to select from `input` and order like. All genes in `reference`'
            ' must be present in `input`.')
    prep_like.add_argument('-o', '--outdir', required=True,
            help='Output directory. Does not need to exist.')
    prep_like.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')
    # other options
    prep_like.add_argument('--by-gene-name', default=False, 
            action='store_true', help='Use gene name rather than ENSEMBL'
            ' id to when matching against reference.  Useful for datasets'
            ' where only gene symbols are given. Used by default when input'
            ' is a loom file (unless there is an Accession attr in the loom).')
    prep_like.add_argument('--no-split-on-dot', default=False, action='store_true',
            help='Don\'t split gene symbol or name on period before'
            ' when matching to reference. We do this by default for ENSEMBL'
            ' ids.')


    ###### Train command
    train = subparsers.add_parser('train',
            help='Train a model with automatic parallelization across'
            ' computations with numba')
    # data and saving
    train.add_argument('-i', '--input', required=True,
            help="Training data. Expects either the mtx file output by the "
            "prep command or a tab-separated tsv file formatted like:" 
            "`CELL_ID\tGENE_ID\tUMI_COUNT`. In the later case, ids are "
            "assumed to be 0 indexed and we assume no duplicates."
            )
    train.add_argument('-o', '--outdir', 
            help='Output directory for scHPF model. Will be created if does '
            'not exist.')
    train.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # Required model hyperparameter
    train.add_argument('-k', '--nfactors', type=int, required=True,
            help='Number of factors.')

    # training parameters
    train.add_argument('-t', '--ntrials',  type=int, default=1,
            help='Number of times to run scHPF, selecting the trial with '
            'best loss (on training data unless validation is given).'
            ' [Default 1]')
    train.add_argument('-v', '--validation-cells', default=None,
            help='Cells to use to assess convergence and choose a model.'
            ' Expects same format as ``-i/--input``. Training data used by'
            ' default.'
            )
    train.add_argument('-M', '--max-iter', type=int, default=1000,
            help='Maximum iterations. [Default 1000].')
    train.add_argument('-m', '--min-iter', type=int, default=30,
            help='Minimum iterations. [Default 30]')
    train.add_argument('-e',  '--epsilon', type=float, default=0.001,
            help='Minimum percent decrease in loss between checks to continue '
            'inference (convergence criteria). [Default 0.001].')
    train.add_argument('-f', '--check-freq', type=int, default=10,
            help='Number of iterations to run between convergence checks. '
            '[Default 10].')
    train.add_argument('--better-than-n-ago', default=5, type=int,
            help= 'Stop condition if loss is getting worse.  Stops training '
            'if loss is worse than `better_than_n_ago`*`check-freq` training '
            'steps ago and getting worse. Normally not necessary to change.')
    train.add_argument('-a', type=float, default=0.3,
            help='Value for hyperparameter a. Setting to -2 will auto-set to'
            ' 1/sqrt(nfactors)[Default 0.3]')
    train.add_argument('-c', type=float, default=0.3,
            help='Value for hyperparameter c. Setting to -2 will auto-set to'
            ' 1/sqrt(nfactors)[Default 0.3]')
    train.add_argument('--float32', action='store_true',
            help="Use 32-bit floats instead of default 64-bit floats in"
            " variational distrubtions")
    train.add_argument('-bs', '--batchsize', default=0, type=int,
            help="Number of cells to use per training round. All cells used if"
            " 0. Note that using batches changes the order of updates during"
            " inference.")
    train.add_argument('-sl', '--smooth-loss', default=1, type=int,
            help="Average loss over the last `--smooth-loss` interations."
            " Intended for when using minibatches, where int(ncells/batchsize)"
            " is a reasonable value"
            )
    train.add_argument('-bts', '--beta-theta-simultaneous', action='store_true',
            help="If False (default), compute beta update, then compute theta"
            " based on the updated beta. Note that if batching is used, this"
            " order is reverse. If True, update both beta and theta based on"
            " values from the last training round. The later slows the rate of"
            " convergence and sometimes results in better log-likelihoods, but"
            " may increase convergence time, especially for large numbers of"
            " cells."
            )
    train.add_argument('-sa', '--save-all', action='store_true',
            help="Save all trials")
    train.add_argument('-rp', '--reproject', action='store_true',
            help="Reproject data onto fixed global (gene) parameters after"
            " convergence, but before model selection. Recommended with"
            " batching")
    train.add_argument('--quiet', dest='verbose', action='store_false', 
            default=True, help="Don't print intermediate llh.")

    ###### train with trials in threadpool
    train_pool = subparsers.add_parser('train-pool', parents=[train], 
            add_help=False, conflict_handler='resolve')
    train_pool.add_argument('--njobs', type=int, default=0,
            help='Max number of processes to spawn. 0 will use the minimum of'
            ' all available cores and ntrials.')
    # Required model hyperparameter
    train_pool.add_argument('-k', '--nfactors', nargs='+', type=int, 
        required=True, help='Number of factors.')


    ### Score command
    score = subparsers.add_parser('score',
            help='Create useful files such as gene scores, cell scores, and'
            ' ranked gene lists in txt format.')
    score.add_argument('-m', '--model', required=True,
            help='Saved scHPF model from train command. Should have extension' 
            '`.joblib`')
    score.add_argument('-o', '--outdir', default=None,
            help='Output directory for score files. If not given, a new'
            ' subdirectory of the dir containing the model will be made with'
            ' the same name as the model file (without extension)')
    score.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')
    score.add_argument('-g', '--genefile', default=None,
            help='Create an additional file with gene names ranked by score '
            'for each factor. Expects the gene.txt file output by the scHPF '
            'prep command or a similarly formatted tab-delimited file without '
            'headers. Uses the zero-indexed ``--name_col``\'th column as gene '
            'names. Optional.')
    score.add_argument('--name-col', type=int, default=1,
            help='The zero-indexed column of `genefile` to use as a gene name '
            'when (optionally) ranking genes. If ``--name_col`` is greater'
            ' than the index of ``--genefile``\'s last column, it is '
            ' automatically reset to the last column\'s index. [Default 1]'
        )



    # ###### Project command
    proj = subparsers.add_parser('project',
            help='Project new data onto a trained model.')
    # data and saving
    proj.add_argument('-m', '--model', required=True,
            help='The model to project onto.')
    proj.add_argument('-i', '--input', required=True,
            help='Data to project onto model. Expects either the mtx file'
            ' output by the prep or prep-like commands or a tab-delimitted'
            ' tsv file formated like: `CELL_ID\tGENE_ID\tUMI_COUNT`. In the'
            ' later case, ids are assumed to be 0 indexed and we assume no'
            ' duplicates.')
    proj.add_argument('-o', '--outdir', 
            help='Output directory for projected scHPF model. Will be created'
            ' if does not exist.')
    proj.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # projection-specific args
    proj.add_argument('--recalc-bp', action='store_true',
            help='Recalculate hyperparameter bp for the new data')

    # Training parameters (same as train, different defaults, no short names)
    proj.add_argument('--max-iter', type=int, default=500,
            help='Maximum iterations. [Default 500].')
    proj.add_argument('--min-iter', type=int, default=10,
            help='Minimum iterations. [Default 10]')
    proj.add_argument('--epsilon', type=float, default=0.001,
            help='Minimum percent decrease in loss between checks to continue '
            'inference (convergence criteria). [Default 0.001].')
    proj.add_argument('--check-freq', type=int, default=10,
            help='Number of iterations to run between convergence checks. '
            '[Default 10].')

    # ###### Select Best command
    select_best = subparsers.add_parser('select-best',
            help='Select the best model from multiple joblib files based on loss.')
    # input and output
    select_best.add_argument('-i', '--input', required=True, nargs='+',
            help='Input joblib files containing scHPF models. All models must'
            ' have been trained with the same parameters.')
    select_best.add_argument('-o', '--outdir', 
            help='Output directory for the selected best model. Will be created'
            ' if does not exist.')
    select_best.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')
    select_best.add_argument('--save-all', action='store_true',
            help="Save all models with their loss values for reference")

    return parser


if __name__=='__main__':
    parser = _parser()
    args = parser.parse_args()

    # print help if no subparser given
    if len(sys.argv)==1:
        parser.print_help(sys.stderr)
        sys.exit(1)

    # setup paths and prefixes

    if args.outdir is None:
        if args.cmd in ['prep', 'prep-like', 'train', 'train-pool']:
            args.outdir = args.input.rsplit('/', 1)[0]
        elif args.cmd=='project':
            args.outdir = args.model.rsplit('/',1)[0]
        elif args.cmd=='score':
            args.outdir = args.model.split('.joblib')[0]
        elif args.cmd=='select-best':
            args.outdir = os.path.dirname(args.input[0]) if len(args.input) > 0 else '.'

    if args.outdir is not None and not os.path.exists(args.outdir):
        print("Creating output directory {} ".format(args.outdir))
        os.makedirs(args.outdir)
    prefix = args.prefix.rstrip('.') + '.' if len(args.prefix) > 0 else ''
    outprefix = args.outdir + '/' +  prefix

    if args.cmd == 'prep':
        filtered, genes = load_and_filter(args.input, 
                min_cells=args.min_cells, 
                whitelist=args.whitelist, 
                blacklist=args.blacklist, 
                filter_by_gene_name=args.filter_by_gene_name,
                no_split_on_dot=args.no_split_on_dot)

        print('Writing filtered data to file.....')
        mmwrite('{}filtered.mtx'.format(outprefix), filtered, field='integer')
        genes.to_csv('{}genes.txt'.format(outprefix), sep='\t', header=None,
                index=None)

        if args.n_validation_cells > 0:
            print('Selecting train/validation cells.....')
            Xtrn, Xvld, vld_ix = split_validation_cells( filtered,
                    args.n_validation_cells, args.validation_group_ids,
                    max_group_frac = args.validation_max_group_frac)
            trn_ix = np.setdiff1d(np.arange(filtered.shape[0]), vld_ix)

            print('Writing train/validation splits.....')
            mmwrite('{}train_cells.mtx'.format(outprefix), Xtrn, 
                    field='integer')
            np.savetxt('{}train_cell_ix.txt'.format(outprefix), trn_ix, 
                    fmt='%d')
            mmwrite('{}validation_cells.mtx'.format(outprefix), Xvld, 
                    field='integer')
            np.savetxt('{}validation_cell_ix.txt'.format(outprefix), vld_ix, 
                    fmt='%d')

        print('Writing commandline arguments to file.....')
        cmdfile = '{}prep_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)



    elif args.cmd == 'prep-like':
        print('Loading and reordering input like reference.... ')
        filtered, genes = load_like(args.input, reference=args.reference,
                by_gene_name=args.by_gene_name, 
                no_split_on_dot=args.no_split_on_dot)
        print('Writing prepared data to file.....')
        mmwrite('{}filtered.mtx'.format(outprefix), filtered, field='integer')
        genes.to_csv('{}genes.txt'.format(outprefix), sep='\t', header=None,
                index=None)
        print('Writing commandline arguments to file.....')
        cmdfile = '{}prep-like_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)


    elif args.cmd in ['train', 'train-pool']:
        # load data
        print( 'Loading data.....' )
        load_fnc = mmread if args.input.endswith('.mtx') else load_coo
        train = load_fnc(args.input)

        ncells, ngenes = train.shape
        msg = '.....found {} cells and {} genes in {}'.format(
                ncells, ngenes, args.input)
        print(msg)

        if args.batchsize and ncells > args.batchsize and not args.reproject:
            msg = '\nWARNING: running with minibatches but without reproject.' \
                + ' We recommend adding the --reproject flag when running with'\
                + ' batches to synchronize cell variational distributions. \n'
            print(msg)

        if args.validation_cells is not None:
            vcells = load_fnc(args.validation_cells)
            msg = '.....found {} validation cells and {} genes in {}'.format(
                    vcells.shape[0], vcells.shape[1], args.validation_cells)
            print(msg)
            msg = 'WARNING: scHPF models with validation cells can be slow'
            msg += ' to converge.\n\tIf you observe this, try either (or both)'
            msg += ' increasing epsilon (-e, currently set to {})'.format(
                    args.epsilon)
            msg += ' or increasing the number of validation cells (using prep)'
            print(msg)
        else:
            vcells = None

        # create model
        print('Running trials.....' )
        dtype = np.float32 if args.float32 else np.float64
        model_kwargs = dict(a=args.a, c=args.c)

        if args.cmd == 'train':
            run_fnc = run_trials
        else:
            if args.njobs < 0:
                msg = 'njobs must be an int >= 0, received {}'
                raise ValueError(msg.format(args.njobs))
            run_fnc = partial(run_trials_pool, njobs=args.njobs)

        # TODO get rid of repeated code
        reject = None
        if args.save_all:
            model, reject = run_fnc(train, vcells=vcells, 
                        nfactors=args.nfactors, ntrials=args.ntrials,
                        min_iter=args.min_iter, max_iter=args.max_iter,
                        check_freq=args.check_freq, epsilon=args.epsilon,
                        better_than_n_ago=args.better_than_n_ago, dtype=dtype,
                        verbose=args.verbose, model_kwargs=model_kwargs,
                        return_all=True, reproject=args.reproject,
                        batchsize=args.batchsize,
                        beta_theta_simultaneous=args.beta_theta_simultaneous,
                        loss_smoothing=args.smooth_loss
                        )
        else:
            model = run_fnc(train, vcells=vcells, nfactors=args.nfactors, 
                        ntrials=args.ntrials, min_iter=args.min_iter,
                        max_iter=args.max_iter, check_freq=args.check_freq,
                        epsilon=args.epsilon,
                        better_than_n_ago=args.better_than_n_ago, dtype=dtype,
                        verbose=args.verbose, model_kwargs=model_kwargs,
                        return_all=False, reproject=args.reproject,
                        batchsize=args.batchsize,
                        beta_theta_simultaneous=args.beta_theta_simultaneous,
                        loss_smoothing=args.smooth_loss
                        )

        # save the model/models
        if isinstance(args.nfactors, int):
            klist = [args.nfactors]
            model = [model]
            if reject is not None:
                reject = [reject]
        else:
            klist = args.nfactors
        for i, (K,m) in enumerate(zip(klist, model)):
            model_outprefix = '{}scHPF_K{}{}_{}trials'.format(
                    outprefix, K, 
                    f'_b{args.batchsize}' if ncells > args.batchsize else '', 
                    args.ntrials)
            if vcells is None:
                print('Saving best model ({} factors).....'.format(K))
                joblib.dump(m, model_outprefix + '.joblib')
            else:
                print('Saving best model (training data, {} factors).....'\
                        .format(K))
                joblib.dump(m, model_outprefix + '.train.joblib')

                print('Computing final validation projection ({} factors)....'\
                        .format(K))
                projection = m.project(vcells, replace=False)
                print('Saving validation projection.....({} factors)'.format(K))
                joblib.dump(projection, 
                        model_outprefix + '.validation_proj.joblib')
            if args.save_all:
                for j,r in enumerate(reject[i]):
                    joblib.dump(r, model_outprefix + f'_reject{j+1}.joblib')


        print('Writing commandline arguments to file.....')
        cmdfile = '{}train_commandline_args.json'.format(outprefix)
        print(cmdfile)
        if os.path.exists(cmdfile):
            cmdfile = '{}train_commandline_args.{}.json'.format(outprefix, 
                    time.strftime("%Y%m%d-%H%M%S"))
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)

        print('\n')


    elif args.cmd == 'score':
        print('Loading model.....')
        model = joblib.load(args.model)

        print('Calculating scores.....')
        cell_score = model.cell_score()
        gene_score = model.gene_score()

        print('Saving scores.....')
        np.savetxt(outprefix + 'cell_score.txt', cell_score, delimiter='\t')
        np.savetxt(outprefix + 'gene_score.txt', gene_score, delimiter='\t')

        print('Calculating mean cellscore fractions.....')
        frac_list = mean_cellscore_fraction_list(cell_score)
        with open(outprefix + 'mean_cellscore_fraction.txt', 'w') as h:
            h.write('nfactors\tmean_cellscore_fraction\n')
            for i,csf in enumerate(frac_list):
                h.write('{}\t{}\n'.format(i+1,csf))

        print('Calculating maximum pairwise overlaps.....')
        table = max_pairwise_table(gene_score, 
                ntop_list=[50,100,150,200,250,300,350,400,450,500])
        table.to_csv(outprefix + 'maximum_overlaps.txt', sep='\t', index=False)

        if args.genefile is not None:
            print('Ranking genes.....')
            # load and format gene file
            genes = np.loadtxt(args.genefile, delimiter='\t', dtype=str)
            if len(genes.shape) == 1:
                genes = genes[:,None]
            # get column to use for gene names
            last_col = genes.shape[1] - 1
            name_col = last_col if args.name_col > last_col else args.name_col
            print('.....using {}\'th column of genefile as gene label'.format(
                name_col))

            # rank the genes by gene_score
            ranks = np.argsort(gene_score, axis=0)[::-1]
            ranked_genes = []
            for i in range(gene_score.shape[1]):
                ranked_genes.append(genes[ranks[:,i], name_col])
            ranked_genes = np.stack(ranked_genes).T
            print('Saving ranked genes.....')
            np.savetxt(outprefix + 'ranked_genes.txt', ranked_genes, 
                    fmt="%s", delimiter='\t')

        print('Writing commandline arguments to file.....')
        cmdfile = '{}score_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2) 

    elif args.cmd == 'project':
        print('Loading reference model.....')
        model = joblib.load(args.model)
        print('Loading data.....')
        load_fnc = mmread if args.input.endswith('.mtx') else load_coo
        proj_data = load_fnc(args.input)
        print('Projecting data.....')
        projection = model.project(proj_data, replace=False, verbose=True,
                                   recalc_bp=args.recalc_bp,
                                   min_iter=args.min_iter, 
                                   max_iter=args.max_iter, 
                                   check_freq=args.check_freq, 
                                   epsilon=args.epsilon, )
        print('Saving projection.....')
        if args.recalc_bp:
            outprefix += '{}.'.format('recalc_bp')
        proj_out = '{}{}.proj.joblib'.format(outprefix, 
                args.model.rsplit('.',1)[0].split('/')[-1])
        joblib.dump(projection, proj_out)

        print('Writing commandline arguments to file.....')
        cmdfile = '{}project_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)

    elif args.cmd == 'select-best':
        print('Loading models from joblib files.....')
        models = []
        losses = []
        filenames = []

        for model_file in args.input:
            try:
                model = joblib.load(model_file)
                models.append(model)
                filenames.append(model_file)

                # Get the final loss value
                if hasattr(model, 'loss') and len(model.loss) > 0:
                    final_loss = model.loss[-1]
                    # Handle case where loss might be a list (from reprojection)
                    if isinstance(final_loss, list):
                        final_loss = final_loss[-1]
                    losses.append(final_loss)
                else:
                    print(f"Warning: Model {model_file} has no loss history")
                    losses.append(float('inf'))

            except Exception as e:
                print(f"Error loading model {model_file}: {e}")
                continue

        if not models:
            print("No valid models could be loaded. Exiting.")
            sys.exit(1)

        print(f'Loaded {len(models)} models successfully.')

        # Check that all models have the same parameters
        print('Verifying model parameter consistency.....')
        reference_model = models[0]
        for i, model in enumerate(models[1:], 1):
            if (model.nfactors != reference_model.nfactors or
                model.a != reference_model.a or
                model.c != reference_model.c or
                model.ap != reference_model.ap or
                model.cp != reference_model.cp or 
                model.version != reference_model.version):
                print(f"Warning: Model {filenames[i]} has different parameters than {filenames[0]}")
                print(f"  nfactors: {model.nfactors} vs {reference_model.nfactors}")
                print(f"  a: {model.a} vs {reference_model.a}")
                print(f"  c: {model.c} vs {reference_model.c}")
                print(f"  ap: {model.ap} vs {reference_model.ap}")
                print(f"  cp: {model.cp} vs {reference_model.cp}")
                print(f"  version: {model.version} vs {reference_model.version}")
                raise ValueError(f"Model parameters are not consistent ({filenames[i]} has different parameters than {filenames[0]})")

        # Find the best model based on loss
        print('Selecting best model based on loss.....')
        best_idx = np.argmin(losses)
        best_model = models[best_idx]
        best_loss = losses[best_idx]
        best_filename = filenames[best_idx]

        print(f'Best model: {best_filename}')
        print(f'Best loss: {best_loss:.6f}')

        # Print all models and their losses
        print('\nAll models and their losses:')
        for i, (filename, loss) in enumerate(zip(filenames, losses)):
            marker = ' *' if i == best_idx else ''
            print(f'  {filename}: {loss:.6f}{marker}')

        # Save the best model
        print('Saving best model.....')
        K = best_model.nfactors
        ncells = best_model.ncells if best_model.ncells else 'unknown'
        batchsize_suffix = f'_b{ncells}' if hasattr(best_model, 'batchsize') and best_model.batchsize else ''

        model_outprefix = '{}scHPF_K{}{}_best'.format(
                outprefix, K, batchsize_suffix)

        print('Saving best model ({} factors).....'.format(K))
        joblib.dump(best_model, model_outprefix + '.joblib')

        # Save all models if requested
        if args.save_all:
            print('Saving all models with loss information.....')
            for i, (model, loss, filename) in enumerate(zip(models, losses, filenames)):
                base_name = os.path.splitext(os.path.basename(filename))[0]
                model_outprefix_all = '{}scHPF_K{}{}_{}_loss{:.6f}'.format(
                        outprefix, K, batchsize_suffix, base_name, loss)
                joblib.dump(model, model_outprefix_all + '.joblib')

        # Save loss comparison summary
        print('Saving loss comparison summary.....')
        summary_data = {
            'filename': filenames,
            'loss': losses,
            'nfactors': [m.nfactors for m in models],
            'ncells': [m.ncells for m in models],
            'ngenes': [m.ngenes for m in models]
        }
        summary_df = pd.DataFrame(summary_data)
        summary_df['is_best'] = summary_df.index == best_idx
        summary_df.to_csv(outprefix + 'model_comparison_summary.csv', index=False)

        print('Writing commandline arguments to file.....')
        cmdfile = '{}select-best_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)
